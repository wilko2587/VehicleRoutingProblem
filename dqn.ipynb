{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_server = './Vehicle-Routing-2/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleRouterEnvironment:\n",
    "\n",
    "    def __init__(self, path=path_server):\n",
    "        self.path = path\n",
    "        files = os.listdir(self.path)\n",
    "        alldata = pd.DataFrame(index=None, columns=range(6))\n",
    "        for filename in files:\n",
    "            with open(os.path.join(self.path, filename)) as f:\n",
    "                data = f.readlines()\n",
    "                numdata = data[9:]\n",
    "                file_data = pd.DataFrame(index=[int(line.split()[0]) for line in numdata],\n",
    "                                         columns=range(6),\n",
    "                                         data=[line.split()[1:] for line in numdata])  # start time at 0\n",
    "                alldata = pd.concat([alldata, file_data], axis=0)\n",
    "\n",
    "        alldata = alldata.astype(\"float\")\n",
    "        dist_scale = alldata.loc[:, [0, 1]].max().max()\n",
    "        time_scale = alldata.loc[:, [3, 4]].max().max()\n",
    "        self._timescale = time_scale\n",
    "        self._distscale = dist_scale\n",
    "        self._speedscaler = 1e-1 # bigger = vehicle moves slower\n",
    "\n",
    "        self.scaler = np.array([dist_scale, dist_scale, 1, time_scale, time_scale])\n",
    "        self._setup(problem_file_index=0, scaler=self.scaler)\n",
    "\n",
    "    def _setup(self, problem_file_index=0, scaler=None):\n",
    "\n",
    "        if scaler is None:\n",
    "            scaler = np.array([1, 1, 1, 1, 1])\n",
    "\n",
    "        files = os.listdir(self.path)\n",
    "        self.problem_file = files[problem_file_index % len(files)]\n",
    "\n",
    "        # read the txt file and load data into a dataframe\n",
    "        with open(os.path.join(self.path, self.problem_file)) as datafile:\n",
    "            data = datafile.readlines()\n",
    "            headers = data[7]\n",
    "            headers = headers.replace(\"CUST NO.\", \"CUST_NO.\")\n",
    "            headers = headers.replace(\"READY TIME\", \"READY_TIME\")\n",
    "            headers = headers.replace(\"DUE DATE\", \"DUE_DATE\")\n",
    "            numdata = data[9:]\n",
    "            self.data = pd.DataFrame(index=[int(line.split()[0]) for line in numdata],\n",
    "                                     columns=headers.split()[1:-1],\n",
    "                                     data=[line.split()[1:] for line in numdata])  # start time at 0\n",
    "            self.data = self.data.astype(\"float\")\n",
    "            self.service_time = self.data[\"SERVICE\"][5]/self._timescale # 5 is random, 0 is always depot\n",
    "            self.data.drop('SERVICE', axis=1, inplace=True)\n",
    "            self.data = self.data.divide(scaler)\n",
    "            self.data['COMPLETED?'] = 0  # keep track of who has been serviced\n",
    "            self.data.loc[0, 'COMPLETED?'] = 1  # by default we say the depot has been serviced\n",
    "\n",
    "        # define the state space and action space\n",
    "        # state is the self.data dataframe + current time + current X coord + current Y coord\n",
    "        self.time = 0  # start clock at 0\n",
    "        self.max_time = self.data[\"DUE_DATE\"].max() * 2\n",
    "\n",
    "        self.XCOORD = self.data[\"XCOORD.\"][0] # depot is different at each file\n",
    "        self.YCOORD = self.data[\"YCOORD.\"][0]\n",
    "\n",
    "        self.action_space = self.data.index.tolist()  # action space is the customer numbers (ie: which customer to visit next)\n",
    "\n",
    "        self._max_episode_steps = 1  # ? figure this is useful + is also in the framework for assignment 4\n",
    "\n",
    "    def reset(self, episode_num):\n",
    "        # refreshes the system to use a new problem file\n",
    "        self._setup(problem_file_index=episode_num, scaler=self.scaler)\n",
    "        return None\n",
    "\n",
    "    def state(self):\n",
    "        # state is 1) dist to customer 0, 2) customer times minus current time\n",
    "        state = self.data.copy()\n",
    "        state.drop('XCOORD.', axis=1, inplace=True)\n",
    "        state.rename(columns={\"YCOORD.\": \"DISTANCE\"}, inplace=True)\n",
    "        state.loc[:, [\"READY_TIME\", \"DUE_DATE\"]] = state.loc[:, [\"READY_TIME\", \"DUE_DATE\"]] - self.time\n",
    "        state.loc[:, [\"DISTANCE\"]] = np.sqrt((self.data.loc[:, \"XCOORD.\"] - self.XCOORD).pow(2) + \\\n",
    "                                             (self.data.loc[:, \"YCOORD.\"] - self.YCOORD).pow(2))\n",
    "        return state.values.flatten()\n",
    "\n",
    "    def step(self, action):\n",
    "        # returns next_state, reward, done, _\n",
    "\n",
    "        # update the self.data (the state)\n",
    "        prior_time, prior_Xcoord, prior_Ycoord = self.time, self.XCOORD, self.YCOORD\n",
    "        next_Xcoord, next_Ycoord = self.data.loc[action][\n",
    "            ['XCOORD.', 'YCOORD.']]  # next coords are defined by the next customer\n",
    "        move_time = np.sqrt((prior_Xcoord - next_Xcoord) ** 2 + (prior_Ycoord - next_Ycoord) ** 2)\n",
    "        move_time = move_time * self._speedscaler * self._distscale / self._timescale\n",
    "        previously_completed = int(self.data.loc[action]['COMPLETED?'])\n",
    "        ready_time = self.data.loc[action]['READY_TIME']\n",
    "        due_date = self.data.loc[action]['DUE_DATE']\n",
    "\n",
    "        if move_time == 0.:\n",
    "            move_time = 1 / self._timescale\n",
    "\n",
    "        # only include service time if we haven't already serviced this customer and if we visited after the ready time\n",
    "        if previously_completed == 0 and prior_time + move_time >= ready_time:\n",
    "            next_time = prior_time + move_time + self.service_time\n",
    "            self.data.loc[action, \"COMPLETED?\"] = 1\n",
    "        else:\n",
    "            next_time = prior_time + move_time\n",
    "\n",
    "        # # work out rewards\n",
    "        # # let reward be +1 if current delivery completed on time AND we arrived to them at least after the ready time\n",
    "        # # -1 reward for every delivery across the system that is late\n",
    "        #reward = 10*int((due_date >= next_time) & (previously_completed == 0) & (prior_time + move_time >= ready_time)) \\\n",
    "        #         - np.sum((self.data.loc[:, 'COMPLETED?'] == 0) & (self.data.loc[:, 'DUE_DATE'] < next_time) * self.data.loc[:, \"DUE_DATE\"] <= next_time) \\\n",
    "        #         - (previously_completed == 1) # subtract if revisiting somewhere\n",
    "\n",
    "        reward = int((due_date >= next_time) & (previously_completed == 0) & (prior_time + move_time >= ready_time)) \\\n",
    "                - np.sum((self.data.loc[:, 'COMPLETED?'] == 0) & (self.data.loc[:, 'DUE_DATE'] < next_time)) / 100\n",
    "\n",
    "        reward = float(reward)#/100 # normalise reward\n",
    "\n",
    "        self.time = next_time\n",
    "        self.XCOORD = next_Xcoord\n",
    "        self.YCOORD = next_Ycoord\n",
    "\n",
    "        # now set the newly serviced customer's COMPLETED? to 1 (True)\n",
    "        done = 0 not in self.data.loc[:, 'COMPLETED?'].tolist()  # done is True if everyone has been serviced\n",
    "\n",
    "        if next_time > self.max_time:   # done also True if max time reached\n",
    "            done = True if next_time > self.max_time else done # done also True if max time reached\n",
    "\n",
    "        # print('new state following action: \\n{}'.format(self.data))\n",
    "        # print('action: {}'.format(action))\n",
    "        # print('time: {}'.format(self.time))\n",
    "        # print('reward: {}'.format(reward))\n",
    "        # print('done? {}'.format(done))\n",
    "\n",
    "        return self.state(), reward, done, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OurModel(input_shape, action_space):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Dense(1028, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    # X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X, name='DQN_model')\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.0025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = VehicleRouterEnvironment()\n",
    "        self.state_size = len(self.env.state())\n",
    "        self.action_size = len(self.env.action_space)\n",
    "        self.EPISODES = 1000\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        \n",
    "        self.gamma = 0.99   # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # create main model\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if len(state.shape) == 1: # catch for dimensionality issues\n",
    "            state = np.array(state).reshape([1, len(state)])\n",
    "\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            return np.random.choice(self.env.action_space)\n",
    "        else:\n",
    "            q_out = self.model.predict(state)\n",
    "            action = np.argmax(q_out)\n",
    "            return action\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "\n",
    "        # Randomly sample minibatch from the memory\n",
    "        t0 = time.time()\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "        #print('1: ', time.time() - t0)\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # assign data into state, next_state, action, reward and done from minibatch\n",
    "        t0 = time.time()\n",
    "        for i in range(self.batch_size):\n",
    "            ministate, miniaction, minireward, mininextstate, minidone = minibatch[i]\n",
    "            state[i] = ministate.reshape([1, self.state_size])\n",
    "            next_state[i] = mininextstate\n",
    "            action.append(self.act(state[i]))\n",
    "            reward.append(minireward)\n",
    "            done.append(minidone)\n",
    "        action = np.array(action)\n",
    "        reward = np.array(reward)\n",
    "        #print('2: ', time.time() - t0)\n",
    "\n",
    "        t0 = time.time()\n",
    "        flat_target = self.model.predict(state).flatten()\n",
    "        Q_target_next = np.max(self.model.predict(next_state), axis=1)\n",
    "        action_indices = action + np.arange(self.batch_size) * self.action_size # provides index in flattened array of where this action maps to\n",
    "        flat_target[action_indices] = reward + self.gamma * Q_target_next.flatten()\n",
    "        target = flat_target.reshape([self.batch_size, self.action_size]) # pull back into shape\n",
    "        #print('3: ', time.time() - t0)\n",
    "\n",
    "        #target = np.zeros((self.batch_size, self.action_size))\n",
    "\n",
    "        ## compute value function of current(call it target) and value function of next state(call it target_next)\n",
    "        #for i in range(self.batch_size):\n",
    "        #    a = action[i]\n",
    "        #    if done[i]:\n",
    "        #        target[i] = self.model.predict(state[i].reshape([1, len(state[i])]))\n",
    "        #        target[i, a] = reward[i]\n",
    "        #    else:\n",
    "        #        Q_target_next = np.max(self.model.predict(next_state[i].reshape([1, len(next_state[i])])))\n",
    "        #        target[i] = self.model.predict(state[i].reshape([1, len(state[i])]))\n",
    "        #        target[i, a] = reward[i] + self.gamma * Q_target_next\n",
    "\n",
    "        t0 = time.time()\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0, epochs=1)\n",
    "        #print('4: ', time.time() - t0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "            \n",
    "    def training(self):\n",
    "        scores = []\n",
    "        stops = []\n",
    "\n",
    "        for e in range(self.EPISODES):\n",
    "            self.env.reset(e)\n",
    "            state = self.env.state()\n",
    "\n",
    "            done = False\n",
    "            i = 0\n",
    "\n",
    "            actions = []\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                actions.append(action)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                if done:\n",
    "                    # dateTimeObj = datetime.now()\n",
    "                    # timestampStr = dateTimeObj.strftime(\"%H:%M:%S\")\n",
    "                    print(\"episode: {}/{}, stops_made: {}, epsilon: {:.2}, total summed reward: {}\".format(e+1, self.EPISODES, i, self.epsilon, total_reward))\n",
    "                    scores.append(total_reward)\n",
    "                    stops.append(i)\n",
    "\n",
    "                #if math.log(i, 10) % 1 > math.log(i+1, 10) % 1: # only replay when the num episodes has crossed an order of magnitude\n",
    "                    # this speeds things up\n",
    "                    # self.replay()\n",
    "                if i % 10 == 0:\n",
    "                    self.replay()\n",
    "\n",
    "        return scores, stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DQN_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 505)]             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               129536    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 101)               13029     \n",
      "=================================================================\n",
      "Total params: 241,253\n",
      "Trainable params: 241,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 1/1000, stops_made: 112, epsilon: 1.0, total summed reward: -4.679999999999998\n",
      "episode: 2/1000, stops_made: 48, epsilon: 1.0, total summed reward: -23.63\n",
      "episode: 3/1000, stops_made: 36, epsilon: 1.0, total summed reward: -17.46\n",
      "episode: 4/1000, stops_made: 225, epsilon: 1.0, total summed reward: -4.0800000000000045\n",
      "episode: 5/1000, stops_made: 146, epsilon: 1.0, total summed reward: -38.11\n",
      "episode: 6/1000, stops_made: 226, epsilon: 1.0, total summed reward: -18.97000000000001\n",
      "episode: 7/1000, stops_made: 245, epsilon: 1.0, total summed reward: -39.48999999999993\n",
      "episode: 8/1000, stops_made: 40, epsilon: 0.99, total summed reward: -10.559999999999997\n",
      "episode: 9/1000, stops_made: 42, epsilon: 0.99, total summed reward: -8.99\n",
      "episode: 10/1000, stops_made: 314, epsilon: 0.96, total summed reward: -70.75000000000004\n",
      "episode: 11/1000, stops_made: 187, epsilon: 0.94, total summed reward: -49.460000000000015\n",
      "episode: 12/1000, stops_made: 313, epsilon: 0.91, total summed reward: -24.489999999999988\n",
      "episode: 13/1000, stops_made: 38, epsilon: 0.91, total summed reward: -16.439999999999998\n",
      "episode: 14/1000, stops_made: 46, epsilon: 0.91, total summed reward: -18.81\n",
      "episode: 15/1000, stops_made: 321, epsilon: 0.88, total summed reward: -37.76000000000009\n",
      "episode: 16/1000, stops_made: 43, epsilon: 0.87, total summed reward: -11.48\n",
      "episode: 17/1000, stops_made: 309, epsilon: 0.85, total summed reward: -20.95000000000002\n",
      "episode: 18/1000, stops_made: 47, epsilon: 0.84, total summed reward: -17.310000000000002\n",
      "episode: 19/1000, stops_made: 51, epsilon: 0.84, total summed reward: -20.779999999999998\n",
      "episode: 20/1000, stops_made: 62, epsilon: 0.84, total summed reward: -31.950000000000003\n",
      "episode: 21/1000, stops_made: 253, epsilon: 0.81, total summed reward: -20.36999999999998\n",
      "episode: 22/1000, stops_made: 50, epsilon: 0.81, total summed reward: -17.160000000000004\n",
      "episode: 23/1000, stops_made: 312, epsilon: 0.79, total summed reward: -27.800000000000026\n",
      "episode: 24/1000, stops_made: 205, epsilon: 0.77, total summed reward: -47.300000000000026\n",
      "episode: 25/1000, stops_made: 54, epsilon: 0.77, total summed reward: -26.019999999999996\n",
      "episode: 26/1000, stops_made: 45, epsilon: 0.76, total summed reward: -15.209999999999996\n",
      "episode: 27/1000, stops_made: 49, epsilon: 0.76, total summed reward: -21.700000000000003\n",
      "episode: 28/1000, stops_made: 344, epsilon: 0.73, total summed reward: -14.930000000000028\n",
      "episode: 29/1000, stops_made: 45, epsilon: 0.73, total summed reward: -17.540000000000003\n",
      "episode: 30/1000, stops_made: 48, epsilon: 0.73, total summed reward: -20.670000000000005\n",
      "episode: 31/1000, stops_made: 155, epsilon: 0.72, total summed reward: -28.839999999999993\n",
      "episode: 32/1000, stops_made: 54, epsilon: 0.71, total summed reward: -14.439999999999998\n",
      "episode: 33/1000, stops_made: 50, epsilon: 0.71, total summed reward: -14.430000000000007\n",
      "episode: 34/1000, stops_made: 54, epsilon: 0.71, total summed reward: -17.73\n",
      "episode: 35/1000, stops_made: 50, epsilon: 0.7, total summed reward: -19.679999999999996\n",
      "episode: 36/1000, stops_made: 169, epsilon: 0.69, total summed reward: -41.930000000000014\n",
      "episode: 37/1000, stops_made: 78, epsilon: 0.69, total summed reward: -30.650000000000006\n",
      "episode: 38/1000, stops_made: 342, epsilon: 0.66, total summed reward: -13.81999999999999\n",
      "episode: 39/1000, stops_made: 47, epsilon: 0.66, total summed reward: -21.31\n",
      "episode: 40/1000, stops_made: 358, epsilon: 0.64, total summed reward: -13.770000000000051\n",
      "episode: 41/1000, stops_made: 369, epsilon: 0.62, total summed reward: -47.61000000000003\n",
      "episode: 42/1000, stops_made: 67, epsilon: 0.61, total summed reward: -24.84999999999999\n",
      "episode: 43/1000, stops_made: 231, epsilon: 0.6, total summed reward: -68.99999999999999\n",
      "episode: 44/1000, stops_made: 63, epsilon: 0.6, total summed reward: -30.299999999999997\n",
      "episode: 45/1000, stops_made: 67, epsilon: 0.59, total summed reward: -34.389999999999986\n",
      "episode: 46/1000, stops_made: 63, epsilon: 0.59, total summed reward: -26.06\n",
      "episode: 47/1000, stops_made: 289, epsilon: 0.57, total summed reward: -86.49999999999994\n",
      "episode: 48/1000, stops_made: 60, epsilon: 0.57, total summed reward: -24.799999999999994\n",
      "episode: 49/1000, stops_made: 375, epsilon: 0.55, total summed reward: -11.12999999999995\n",
      "episode: 50/1000, stops_made: 257, epsilon: 0.53, total summed reward: -14.289999999999988\n",
      "episode: 51/1000, stops_made: 314, epsilon: 0.52, total summed reward: -68.05999999999982\n",
      "episode: 52/1000, stops_made: 74, epsilon: 0.51, total summed reward: -37.03\n",
      "episode: 53/1000, stops_made: 271, epsilon: 0.5, total summed reward: -84.08999999999996\n",
      "episode: 54/1000, stops_made: 416, epsilon: 0.48, total summed reward: -84.71999999999991\n",
      "episode: 55/1000, stops_made: 305, epsilon: 0.47, total summed reward: -75.66999999999987\n",
      "episode: 56/1000, stops_made: 79, epsilon: 0.46, total summed reward: -35.00000000000001\n",
      "episode: 57/1000, stops_made: 248, epsilon: 0.45, total summed reward: -54.97000000000008\n",
      "episode: 58/1000, stops_made: 66, epsilon: 0.45, total summed reward: -31.309999999999995\n",
      "episode: 59/1000, stops_made: 90, epsilon: 0.45, total summed reward: -40.72\n",
      "episode: 60/1000, stops_made: 340, epsilon: 0.43, total summed reward: -60.80000000000011\n",
      "episode: 61/1000, stops_made: 276, epsilon: 0.42, total summed reward: -76.69999999999999\n",
      "episode: 62/1000, stops_made: 301, epsilon: 0.41, total summed reward: -78.4299999999999\n",
      "episode: 63/1000, stops_made: 368, epsilon: 0.39, total summed reward: -97.59000000000003\n",
      "episode: 64/1000, stops_made: 66, epsilon: 0.39, total summed reward: -31.230000000000008\n",
      "episode: 65/1000, stops_made: 85, epsilon: 0.39, total summed reward: -38.019999999999996\n",
      "episode: 66/1000, stops_made: 450, epsilon: 0.37, total summed reward: -129.7300000000001\n",
      "episode: 67/1000, stops_made: 364, epsilon: 0.36, total summed reward: -108.72000000000008\n",
      "episode: 68/1000, stops_made: 477, epsilon: 0.34, total summed reward: -91.64000000000004\n",
      "episode: 69/1000, stops_made: 72, epsilon: 0.34, total summed reward: -33.51\n",
      "episode: 70/1000, stops_made: 64, epsilon: 0.34, total summed reward: -34.22999999999999\n",
      "episode: 71/1000, stops_made: 466, epsilon: 0.32, total summed reward: -95.64000000000016\n",
      "episode: 72/1000, stops_made: 84, epsilon: 0.32, total summed reward: -37.75000000000002\n",
      "episode: 73/1000, stops_made: 477, epsilon: 0.3, total summed reward: -87.96\n",
      "episode: 74/1000, stops_made: 91, epsilon: 0.3, total summed reward: -49.67\n",
      "episode: 75/1000, stops_made: 100, epsilon: 0.3, total summed reward: -56.11000000000001\n",
      "episode: 76/1000, stops_made: 128, epsilon: 0.29, total summed reward: -78.08000000000003\n",
      "episode: 77/1000, stops_made: 381, epsilon: 0.28, total summed reward: -77.7999999999999\n",
      "episode: 78/1000, stops_made: 93, epsilon: 0.28, total summed reward: -47.47\n",
      "episode: 79/1000, stops_made: 529, epsilon: 0.27, total summed reward: -115.32999999999971\n",
      "episode: 80/1000, stops_made: 363, epsilon: 0.26, total summed reward: -113.44999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 81/1000, stops_made: 136, epsilon: 0.25, total summed reward: -62.530000000000015\n",
      "episode: 82/1000, stops_made: 74, epsilon: 0.25, total summed reward: -33.99999999999999\n",
      "episode: 83/1000, stops_made: 105, epsilon: 0.25, total summed reward: -65.66000000000001\n",
      "episode: 84/1000, stops_made: 539, epsilon: 0.24, total summed reward: -100.30000000000015\n",
      "episode: 85/1000, stops_made: 105, epsilon: 0.23, total summed reward: -61.50000000000001\n",
      "episode: 86/1000, stops_made: 121, epsilon: 0.23, total summed reward: -49.12000000000002\n",
      "episode: 87/1000, stops_made: 371, epsilon: 0.22, total summed reward: -112.76999999999981\n",
      "episode: 88/1000, stops_made: 83, epsilon: 0.22, total summed reward: -29.040000000000003\n",
      "episode: 89/1000, stops_made: 114, epsilon: 0.22, total summed reward: -50.65000000000002\n",
      "episode: 90/1000, stops_made: 94, epsilon: 0.22, total summed reward: -38.37\n",
      "episode: 91/1000, stops_made: 104, epsilon: 0.21, total summed reward: -51.860000000000014\n",
      "episode: 92/1000, stops_made: 391, epsilon: 0.21, total summed reward: -138.80000000000015\n",
      "episode: 93/1000, stops_made: 178, epsilon: 0.2, total summed reward: -89.44000000000005\n",
      "episode: 94/1000, stops_made: 564, epsilon: 0.19, total summed reward: -104.69000000000007\n",
      "episode: 95/1000, stops_made: 128, epsilon: 0.19, total summed reward: -64.22000000000001\n",
      "episode: 96/1000, stops_made: 605, epsilon: 0.18, total summed reward: -64.47000000000003\n",
      "episode: 97/1000, stops_made: 578, epsilon: 0.17, total summed reward: -150.57999999999998\n",
      "episode: 98/1000, stops_made: 111, epsilon: 0.17, total summed reward: -57.98999999999998\n",
      "episode: 99/1000, stops_made: 582, epsilon: 0.16, total summed reward: -189.4099999999997\n",
      "episode: 100/1000, stops_made: 108, epsilon: 0.16, total summed reward: -61.900000000000006\n",
      "episode: 101/1000, stops_made: 161, epsilon: 0.15, total summed reward: -75.07000000000002\n",
      "episode: 102/1000, stops_made: 115, epsilon: 0.15, total summed reward: -53.64999999999997\n",
      "episode: 103/1000, stops_made: 552, epsilon: 0.14, total summed reward: -218.59000000000015\n",
      "episode: 104/1000, stops_made: 151, epsilon: 0.14, total summed reward: -67.18000000000005\n",
      "episode: 105/1000, stops_made: 670, epsilon: 0.13, total summed reward: -117.42999999999977\n",
      "episode: 106/1000, stops_made: 533, epsilon: 0.13, total summed reward: -108.67000000000003\n",
      "episode: 107/1000, stops_made: 604, epsilon: 0.12, total summed reward: -191.53000000000011\n",
      "episode: 108/1000, stops_made: 246, epsilon: 0.12, total summed reward: -122.51999999999995\n",
      "episode: 109/1000, stops_made: 581, epsilon: 0.11, total summed reward: -212.12000000000046\n",
      "episode: 110/1000, stops_made: 721, epsilon: 0.1, total summed reward: -206.82999999999993\n",
      "episode: 111/1000, stops_made: 622, epsilon: 0.095, total summed reward: -218.24999999999986\n",
      "episode: 112/1000, stops_made: 150, epsilon: 0.094, total summed reward: -78.88999999999994\n",
      "episode: 113/1000, stops_made: 498, epsilon: 0.089, total summed reward: -149.01000000000025\n",
      "episode: 114/1000, stops_made: 161, epsilon: 0.088, total summed reward: -91.54000000000006\n",
      "episode: 115/1000, stops_made: 189, epsilon: 0.086, total summed reward: -112.60999999999996\n",
      "episode: 116/1000, stops_made: 671, epsilon: 0.081, total summed reward: -194.22000000000003\n",
      "episode: 117/1000, stops_made: 690, epsilon: 0.075, total summed reward: -219.55000000000007\n",
      "episode: 118/1000, stops_made: 687, epsilon: 0.07, total summed reward: -219.40000000000043\n",
      "episode: 119/1000, stops_made: 704, epsilon: 0.066, total summed reward: -251.05999999999992\n",
      "episode: 120/1000, stops_made: 151, epsilon: 0.065, total summed reward: -73.83000000000006\n",
      "episode: 121/1000, stops_made: 164, epsilon: 0.064, total summed reward: -82.52999999999993\n",
      "episode: 122/1000, stops_made: 864, epsilon: 0.058, total summed reward: -321.79000000000025\n",
      "episode: 123/1000, stops_made: 911, epsilon: 0.053, total summed reward: -293.9600000000014\n",
      "episode: 124/1000, stops_made: 833, epsilon: 0.049, total summed reward: -217.88999999999947\n",
      "episode: 125/1000, stops_made: 261, epsilon: 0.048, total summed reward: -129.1000000000001\n",
      "episode: 126/1000, stops_made: 183, epsilon: 0.047, total summed reward: -111.09999999999995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-617926243d50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-7170c16684f9>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0;31m# self.replay()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7170c16684f9>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mministate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmininextstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminireward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminidone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7170c16684f9>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mq_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     \"\"\"\n\u001b[1;32m   1620\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3986\u001b[0m         \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3987\u001b[0m         \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3988\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   3989\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMapDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, name)\u001b[0m\n\u001b[1;32m   2799\u001b[0m         \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m         \u001b[0;34m\"use_inter_op_parallelism\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2801\u001b[0;31m         \"preserve_cardinality\", preserve_cardinality)\n\u001b[0m\u001b[1;32m   2802\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2803\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "scores,stops = agent.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r202.txt'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.env.problem_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(len(scores))\n",
    "plt.plot(episodes, scores)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episodes, stops)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Stops')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
