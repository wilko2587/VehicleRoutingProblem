{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADM_VRP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \"\"\" Attention Layer - multi-head scaled dot product attention (for encoder and decoder)\n",
        "\n",
        "        Args:\n",
        "            num_heads: number of attention heads which will be computed in parallel\n",
        "            d_model: embedding size of output features\n",
        "\n",
        "        Call arguments:\n",
        "            q: query, shape (..., seq_len_q, depth_q)\n",
        "            k: key, shape == (..., seq_len_k, depth_k)\n",
        "            v: value, shape == (..., seq_len_v, depth_v)\n",
        "            mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k) or None.\n",
        "\n",
        "            Since we use scaled-product attention, we assume seq_len_k = seq_len_v\n",
        "\n",
        "        Returns:\n",
        "              attention outputs of shape (batch_size, seq_len_q, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_heads, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_depth = self.d_model // self.n_heads\n",
        "\n",
        "        if self.d_model % self.n_heads != 0:\n",
        "            raise ValueError(\"number of heads must divide d_model\")\n",
        "\n",
        "        # define weight matrices\n",
        "        self.wq = tf.keras.layers.Dense(self.d_model, use_bias=False)  # (d_q, d_model)\n",
        "        self.wk = tf.keras.layers.Dense(self.d_model, use_bias=False)  # (d_k, d_model)\n",
        "        self.wv = tf.keras.layers.Dense(self.d_model, use_bias=False)  # (d_v, d_model)\n",
        "\n",
        "        self.w_out = tf.keras.layers.Dense(self.d_model, use_bias=False)  # (d_model, d_model)\n",
        "\n",
        "    def split_heads(self, tensor, batch_size):\n",
        "        \"\"\"Function for computing attention on several heads simultaneously\n",
        "        Splits last dimension of a tensor into (num_heads, head_depth).\n",
        "        Then we transpose it as (batch_size, num_heads, ..., head_depth) so that we can use broadcast\n",
        "        \"\"\"\n",
        "        tensor = tf.reshape(tensor, (batch_size, -1, self.n_heads, self.head_depth))\n",
        "        return tf.transpose(tensor, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # treats first parameter q as input, and  k, v as parameters, so input_shape=q.shape\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        # shape of q: (batch_size, seq_len_q, d_q)\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # compute Q = q * w_q, ...\n",
        "        Q = self.wq(q)  # (batch_size, seq_len_q, d_q) x (d_q, d_model) --> (batch_size, seq_len_q, d_model)\n",
        "        K = self.wk(k)  # ... --> (batch_size, seq_len_k, d_model)\n",
        "        V = self.wv(v)  # ... --> (batch_size, seq_len_v, d_model)\n",
        "\n",
        "        # split heads: d_model = num_heads * head_depth + reshape\n",
        "        Q = self.split_heads(Q, batch_size)  # (batch_size, num_heads, seq_len_q, head_depth)\n",
        "        K = self.split_heads(K, batch_size)  # (batch_size, num_heads, seq_len_k, head_depth)\n",
        "        V = self.split_heads(V, batch_size)  # (batch_size, num_heads, seq_len_v, head_depth)\n",
        "\n",
        "        # similarity between context vector Q and key K // self-similarity in case of self-attention\n",
        "        compatibility = tf.matmul(Q, K, transpose_b=True)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "                                                           # seq_len_q = n_nodes for encoder self-attention\n",
        "                                                           # seq_len_q = 1 for decoder context-vector attention\n",
        "                                                           # seq_len_k = n_nodes for both encoder & decoder\n",
        "        # rescaling\n",
        "        dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
        "        compatibility = compatibility / tf.math.sqrt(dk)\n",
        "\n",
        "        if mask is not None:\n",
        "            # we need to reshape mask:\n",
        "            # (batch_size, seq_len_q, seq_len_k) --> (batch_size, 1, seq_len_q, seq_len_k)\n",
        "            # so that we will be able to do a broadcast:\n",
        "            # (batch_size, num_heads, seq_len_q, seq_len_k) + (batch_size, 1, seq_len_q, seq_len_k)\n",
        "            mask = mask[:, tf.newaxis, :, :]\n",
        "\n",
        "            # we use tf.where since 0*-np.inf returns nan, but not -np.inf\n",
        "            # compatibility = tf.where(\n",
        "            #                     tf.broadcast_to(mask, compatibility.shape), tf.ones_like(compatibility) * (-np.inf),\n",
        "            #                     compatibility\n",
        "            #                      )\n",
        "\n",
        "            compatibility = tf.where(mask,\n",
        "                                    tf.ones_like(compatibility) * (-np.inf),\n",
        "                                    compatibility)\n",
        "\n",
        "        compatibility = tf.nn.softmax(compatibility, axis=-1)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "\n",
        "        # Replace NaN by zeros (tf.nn.softmax returns NaNs for masked rows)\n",
        "        compatibility = tf.where(tf.math.is_nan(compatibility), tf.zeros_like(compatibility), compatibility)\n",
        "\n",
        "        # seq_len_k = seq_len_v\n",
        "        attention = tf.matmul(compatibility, V)  # (batch_size, num_heads, seq_len_q, head_depth)\n",
        "\n",
        "        # transpose back to (batch_size, seq_len_q, num_heads, head_depth)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # concatenate heads (last 2 dimensions)\n",
        "        attention = tf.reshape(attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # project output to the same dimension\n",
        "        # this is equiv. to sum in the article (project heads with W_o and sum), beacuse of block-matrix multiplication\n",
        "        #e.g. https://math.stackexchange.com/questions/2961550/matrix-block-multiplication-definition-properties-and-applications\n",
        "\n",
        "        output = self.w_out(attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "_566zXq1oJFY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#from layers import MultiHeadAttention\n",
        "\n",
        "\n",
        "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"Feed-Forward Sublayer: fully-connected Feed-Forward network,\n",
        "    built based on MHA vectors from MultiHeadAttention layer with skip-connections\n",
        "\n",
        "        Args:\n",
        "            num_heads: number of attention heads in MHA layers.\n",
        "            input_dim: embedding size that will be used as d_model in MHA layers.\n",
        "            feed_forward_hidden: number of neuron units in each FF layer.\n",
        "\n",
        "        Call arguments:\n",
        "            x: batch of shape (batch_size, n_nodes, node_embedding_size).\n",
        "            mask: mask for MHA layer\n",
        "\n",
        "        Returns:\n",
        "               outputs of shape (batch_size, n_nodes, input_dim)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, feed_forward_hidden=512, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mha = MultiHeadAttention(n_heads=num_heads, d_model=input_dim, name='MHA')\n",
        "        self.ff1 = tf.keras.layers.Dense(feed_forward_hidden, name='ff1')\n",
        "        self.ff2 = tf.keras.layers.Dense(input_dim, name='ff2')\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        mha_out = self.mha(x, x, x, mask)\n",
        "        sc1_out = tf.keras.layers.Add()([x, mha_out])\n",
        "        tanh1_out = tf.keras.activations.tanh(sc1_out)\n",
        "\n",
        "        ff1_out = self.ff1(tanh1_out)\n",
        "        relu1_out = tf.keras.activations.relu(ff1_out)\n",
        "        ff2_out = self.ff2(relu1_out)\n",
        "        sc2_out = tf.keras.layers.Add()([tanh1_out, ff2_out])\n",
        "        tanh2_out = tf.keras.activations.tanh(sc2_out)\n",
        "\n",
        "        return tanh2_out\n",
        "\n",
        "class GraphAttentionEncoder(tf.keras.layers.Layer):\n",
        "    \"\"\"Graph Encoder, which uses MultiHeadAttentionLayer sublayer.\n",
        "\n",
        "        Args:\n",
        "            input_dim: embedding size that will be used as d_model in MHA layers.\n",
        "            num_heads: number of attention heads in MHA layers.\n",
        "            num_layers: number of attention layers that will be used in encoder.\n",
        "            feed_forward_hidden: number of neuron units in each FF layer.\n",
        "\n",
        "        Call arguments:\n",
        "            x: tuples of 3 tensors:  (batch_size, 2), (batch_size, n_nodes-1, 2), (batch_size, n_nodes-1)\n",
        "            First tensor contains coordinates for depot, second one is for coordinates of other nodes,\n",
        "            Last tensor is for normalized demands for nodes except depot\n",
        "\n",
        "            mask: mask for MHA layer\n",
        "\n",
        "        Returns:\n",
        "               Embedding for all nodes + mean embedding for graph.\n",
        "               Tuples ((batch_size, n_nodes, input_dim), (batch_size, input_dim))\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, num_layers, feed_forward_hidden=512):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.feed_forward_hidden = feed_forward_hidden\n",
        "\n",
        "        # initial embeddings (batch_size, n_nodes-1, 2) --> (batch-size, input_dim), separate for depot and other nodes\n",
        "        self.init_embed_depot = tf.keras.layers.Dense(self.input_dim, name='init_embed_depot')  # nn.Linear(2, embedding_dim)\n",
        "        self.init_embed = tf.keras.layers.Dense(self.input_dim, name='init_embed')\n",
        "\n",
        "        self.mha_layers = [MultiHeadAttentionLayer(self.input_dim, self.num_heads, self.feed_forward_hidden)\n",
        "                            for _ in range(self.num_layers)]\n",
        "\n",
        "    def call(self, x, mask=None, cur_num_nodes=None):\n",
        "\n",
        "        x = tf.concat((self.init_embed_depot(x[0])[:, None, :],  # (batch_size, 2) --> (batch_size, 1, 2)\n",
        "                       self.init_embed(tf.concat((x[1], x[2][:, :, None]), axis=-1))  # (batch_size, n_nodes-1, 2) + (batch_size, n_nodes-1)\n",
        "                       ), axis=1)  # (batch_size, n_nodes, input_dim)\n",
        "\n",
        "        # stack attention layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.mha_layers[i](x, mask)\n",
        "\n",
        "        if mask is not None:\n",
        "            output = (x, tf.reduce_sum(x, axis=1) / cur_num_nodes)\n",
        "        else:\n",
        "            output = (x, tf.reduce_mean(x, axis=1))\n",
        "            \n",
        "        return output # (embeds of nodes, avg graph embed)=((batch_size, n_nodes, input), (batch_size, input_dim))\n"
      ],
      "metadata": {
        "id": "B50BjmDGoEaU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class AgentVRP():\n",
        "\n",
        "    VEHICLE_CAPACITY = 1.0\n",
        "\n",
        "    def __init__(self, input):\n",
        "\n",
        "        depot = input[0]\n",
        "        loc = input[1]\n",
        "\n",
        "        self.batch_size, self.n_loc, _ = loc.shape  # (batch_size, n_nodes, 2)\n",
        "\n",
        "        # Coordinates of depot + other nodes\n",
        "        self.coords = tf.concat((depot[:, None, :], loc), -2)\n",
        "        self.demand = tf.cast(input[2], tf.float32)\n",
        "\n",
        "        # Indices of graphs in batch\n",
        "        self.ids = tf.range(self.batch_size, dtype=tf.int64)[:, None]\n",
        "\n",
        "        # State\n",
        "        self.prev_a = tf.zeros((self.batch_size, 1), dtype=tf.float32)\n",
        "        self.from_depot = self.prev_a == 0\n",
        "        self.used_capacity = tf.zeros((self.batch_size, 1), dtype=tf.float32)\n",
        "\n",
        "        # Nodes that have been visited will be marked with 1\n",
        "        self.visited = tf.zeros((self.batch_size, 1, self.n_loc + 1), dtype=tf.uint8)\n",
        "\n",
        "        # Step counter\n",
        "        self.i = tf.zeros(1, dtype=tf.int64)\n",
        "\n",
        "        # Constant tensors for scatter update (in step method)\n",
        "        self.step_updates = tf.ones((self.batch_size, 1), dtype=tf.uint8)  # (batch_size, 1)\n",
        "        self.scatter_zeros = tf.zeros((self.batch_size, 1), dtype=tf.int64)  # (batch_size, 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def outer_pr(a, b):\n",
        "        \"\"\"Outer product of matrices\n",
        "        \"\"\"\n",
        "        return tf.einsum('ki,kj->kij', a, b)\n",
        "\n",
        "    def get_att_mask(self):\n",
        "        \"\"\" Mask (batch_size, n_nodes, n_nodes) for attention encoder.\n",
        "            We mask already visited nodes except depot\n",
        "        \"\"\"\n",
        "\n",
        "        # We dont want to mask depot\n",
        "        att_mask = tf.squeeze(tf.cast(self.visited, tf.float32), axis=-2)[:, 1:]  # [batch_size, 1, n_nodes] --> [batch_size, n_nodes-1]\n",
        "        \n",
        "        # Number of nodes in new instance after masking\n",
        "        cur_num_nodes = self.n_loc + 1 - tf.reshape(tf.reduce_sum(att_mask, -1), (-1,1))  # [batch_size, 1]\n",
        "        \n",
        "        att_mask = tf.concat((tf.zeros(shape=(att_mask.shape[0],1),dtype=tf.float32),att_mask), axis=-1)\n",
        "\n",
        "        ones_mask = tf.ones_like(att_mask)\n",
        "\n",
        "        # Create square attention mask from row-like mask\n",
        "        att_mask = AgentVRP.outer_pr(att_mask, ones_mask) \\\n",
        "                            + AgentVRP.outer_pr(ones_mask, att_mask)\\\n",
        "                            - AgentVRP.outer_pr(att_mask, att_mask)\n",
        "        \n",
        "        return tf.cast(att_mask, dtype=tf.bool), cur_num_nodes\n",
        "\n",
        "    def all_finished(self):\n",
        "        \"\"\"Checks if all games are finished\n",
        "        \"\"\"\n",
        "        return tf.reduce_all(tf.cast(self.visited, tf.bool))\n",
        "\n",
        "    def partial_finished(self):\n",
        "        \"\"\"Checks if partial solution for all graphs has been built, i.e. all agents came back to depot\n",
        "        \"\"\"\n",
        "        return tf.reduce_all(self.from_depot) and self.i != 0\n",
        "\n",
        "    def get_mask(self):\n",
        "        \"\"\" Returns a mask (batch_size, 1, n_nodes) with available actions.\n",
        "            Impossible nodes are masked.\n",
        "        \"\"\"\n",
        "\n",
        "        # Exclude depot\n",
        "        visited_loc = self.visited[:, :, 1:]\n",
        "\n",
        "        # Mark nodes which exceed vehicle capacity\n",
        "        exceeds_cap = self.demand + self.used_capacity > self.VEHICLE_CAPACITY\n",
        "\n",
        "        # We mask nodes that are already visited or have too much demand\n",
        "        # Also for dynamical model we stop agent at depot when it arrives there (for partial solution)\n",
        "        mask_loc = tf.cast(visited_loc, tf.bool) | exceeds_cap[:, None, :] | ((self.i > 0) & self.from_depot[:, None, :])\n",
        "\n",
        "        # We can choose depot if 1) we are not in depot OR 2) all nodes are visited\n",
        "        mask_depot = self.from_depot & (tf.reduce_sum(tf.cast(mask_loc == False, tf.int32), axis=-1) > 0)\n",
        "\n",
        "        return tf.concat([mask_depot[:, :, None], mask_loc], axis=-1)\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        # Update current state\n",
        "        selected = action[:, None]\n",
        "\n",
        "        self.prev_a = selected\n",
        "        self.from_depot = self.prev_a == 0\n",
        "\n",
        "        # We have to shift indices by 1 since demand doesn't include depot\n",
        "        # 0-index in demand corresponds to the FIRST node\n",
        "        selected_demand = tf.gather_nd(self.demand,\n",
        "                                       tf.concat([self.ids, tf.clip_by_value(self.prev_a - 1, 0, self.n_loc - 1)], axis=1)\n",
        "                                       )[:, None]  # (batch_size, 1)\n",
        "\n",
        "        # We add current node capacity to used capacity and set it to zero if we return to the depot\n",
        "        self.used_capacity = (self.used_capacity + selected_demand) * (1.0 - tf.cast(self.from_depot, tf.float32))\n",
        "\n",
        "        # Update visited nodes (set 1 to visited nodes)\n",
        "        idx = tf.cast(tf.concat((self.ids, self.scatter_zeros, self.prev_a), axis=-1), tf.int32)[:, None, :]  # (batch_size, 1, 3)\n",
        "        self.visited = tf.tensor_scatter_nd_update(self.visited, idx, self.step_updates)  # (batch_size, 1, n_nodes)\n",
        "\n",
        "        self.i = self.i + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def get_costs(dataset, pi):\n",
        "\n",
        "        # Place nodes with coordinates in order of decoder tour\n",
        "        loc_with_depot = tf.concat([dataset[0][:, None, :], dataset[1]], axis=1)  # (batch_size, n_nodes, 2)\n",
        "        d = tf.gather(loc_with_depot, tf.cast(pi, tf.int32), batch_dims=1)\n",
        "\n",
        "        # Calculation of total distance\n",
        "        # Note: first element of pi is not depot, but the first selected node in the path\n",
        "        return (tf.reduce_sum(tf.norm(d[:, 1:] - d[:, :-1], ord=2, axis=2), axis=1)\n",
        "                + tf.norm(d[:, 0] - dataset[0], ord=2, axis=1) # Distance from depot to first selected node\n",
        "                + tf.norm(d[:, -1] - dataset[0], ord=2, axis=1))  # Distance from last selected node (!=0 for graph with longest path) to depot\n"
      ],
      "metadata": {
        "id": "wU0PNnzSoPo0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#from attention_graph_encoder import GraphAttentionEncoder\n",
        "#from enviroment import AgentVRP\n",
        "\n",
        "\n",
        "def set_decode_type(model, decode_type):\n",
        "    model.set_decode_type(decode_type)\n",
        "\n",
        "class AttentionDynamicModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_dim,\n",
        "                 n_encode_layers=2,\n",
        "                 n_heads=8,\n",
        "                 tanh_clipping=10.\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # attributes for MHA\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_encode_layers = n_encode_layers\n",
        "        self.decode_type = None\n",
        "\n",
        "        # attributes for VRP problem\n",
        "        self.problem = AgentVRP\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # Encoder part\n",
        "        self.embedder = GraphAttentionEncoder(input_dim=self.embedding_dim,\n",
        "                                              num_heads=self.n_heads,\n",
        "                                              num_layers=self.n_encode_layers\n",
        "                                              )\n",
        "\n",
        "        # Decoder part\n",
        "\n",
        "        self.output_dim = self.embedding_dim\n",
        "        self.num_heads = n_heads\n",
        "\n",
        "        self.head_depth = self.output_dim // self.num_heads\n",
        "        self.dk_mha_decoder = tf.cast(self.head_depth, tf.float32)  # for decoding in mha_decoder\n",
        "        self.dk_get_loc_p = tf.cast(self.output_dim, tf.float32)  # for decoding in mha_decoder\n",
        "\n",
        "        if self.output_dim % self.num_heads != 0:\n",
        "            raise ValueError(\"number of heads must divide d_model=output_dim\")\n",
        "\n",
        "        self.tanh_clipping = tanh_clipping\n",
        "\n",
        "        # we split projection matrix Wq into 2 matrices: Wq*[h_c, h_N, D] = Wq_context*h_c + Wq_step_context[h_N, D]\n",
        "        self.wq_context = tf.keras.layers.Dense(self.output_dim, use_bias=False,\n",
        "                                                name='wq_context')  # (d_q_context, output_dim)\n",
        "        self.wq_step_context = tf.keras.layers.Dense(self.output_dim, use_bias=False,\n",
        "                                                     name='wq_step_context')  # (d_q_step_context, output_dim)\n",
        "\n",
        "        # we need two Wk projections since there is MHA followed by 1-head attention - they have different keys K\n",
        "        self.wk = tf.keras.layers.Dense(self.output_dim, use_bias=False, name='wk')  # (d_k, output_dim)\n",
        "        self.wk_tanh = tf.keras.layers.Dense(self.output_dim, use_bias=False, name='wk_tanh')  # (d_k_tanh, output_dim)\n",
        "\n",
        "        # we dont need Wv projection for 1-head attention: only need attention weights as outputs\n",
        "        self.wv = tf.keras.layers.Dense(self.output_dim, use_bias=False, name='wv')  # (d_v, output_dim)\n",
        "\n",
        "        # we dont need wq for 1-head tanh attention, since we can absorb it into w_out\n",
        "        self.w_out = tf.keras.layers.Dense(self.output_dim, use_bias=False, name='w_out')  # (d_model, d_model)\n",
        "\n",
        "    def set_decode_type(self, decode_type):\n",
        "        self.decode_type = decode_type\n",
        "\n",
        "    def split_heads(self, tensor, batch_size):\n",
        "        \"\"\"Function for computing attention on several heads simultaneously\n",
        "        Splits last dimension of a tensor into (num_heads, head_depth).\n",
        "        Then we transpose it as (batch_size, num_heads, ..., head_depth) so that we can use broadcast\n",
        "        \"\"\"\n",
        "        tensor = tf.reshape(tensor, (batch_size, -1, self.num_heads, self.head_depth))\n",
        "        return tf.transpose(tensor, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def _select_node(self, logits):\n",
        "        \"\"\"Select next node based on decoding type.\n",
        "        \"\"\"\n",
        "\n",
        "        # assert tf.reduce_all(logits == logits), \"Probs should not contain any nans\"\n",
        "\n",
        "        if self.decode_type == \"greedy\":\n",
        "            selected = tf.math.argmax(logits, axis=-1)  # (batch_size, 1)\n",
        "\n",
        "        elif self.decode_type == \"sampling\":\n",
        "            # logits has a shape of (batch_size, 1, n_nodes), we have to squeeze it\n",
        "            # to (batch_size, n_nodes) since tf.random.categorical requires matrix\n",
        "            selected = tf.random.categorical(logits[:, 0, :], 1)  # (bach_size,1)\n",
        "        else:\n",
        "            assert False, \"Unknown decode type\"\n",
        "\n",
        "        return tf.squeeze(selected, axis=-1)  # (bach_size,)\n",
        "\n",
        "    def get_step_context(self, state, embeddings):\n",
        "        \"\"\"Takes a state and graph embeddings,\n",
        "           Returns a part [h_N, D] of context vector [h_c, h_N, D],\n",
        "           that is related to RL Agent last step.\n",
        "        \"\"\"\n",
        "        # index of previous node\n",
        "        prev_node = state.prev_a  # (batch_size, 1)\n",
        "\n",
        "        # from embeddings=(batch_size, n_nodes, input_dim) select embeddings of previous nodes\n",
        "        cur_embedded_node = tf.gather(embeddings, tf.cast(prev_node, tf.int32), batch_dims=1)  # (batch_size, 1, input_dim)\n",
        "\n",
        "        # add remaining capacity\n",
        "        step_context = tf.concat([cur_embedded_node, self.problem.VEHICLE_CAPACITY - state.used_capacity[:, :, None]], axis=-1)\n",
        "\n",
        "        return step_context  # (batch_size, 1, input_dim + 1)\n",
        "\n",
        "    def decoder_mha(self, Q, K, V, mask=None):\n",
        "        \"\"\" Computes Multi-Head Attention part of decoder\n",
        "        Basically, its a part of MHA sublayer, but we cant construct a layer since Q changes in a decoding loop.\n",
        "\n",
        "        Args:\n",
        "            mask: a mask for visited nodes,\n",
        "                has shape (batch_size, seq_len_q, seq_len_k), seq_len_q = 1 for context vector attention in decoder\n",
        "            Q: query (context vector for decoder)\n",
        "                    has shape (..., seq_len_q, head_depth) with seq_len_q = 1 for context_vector attention in decoder\n",
        "            K, V: key, value (projections of nodes embeddings)\n",
        "                have shape (..., seq_len_k, head_depth), (..., seq_len_v, head_depth),\n",
        "                                                                with seq_len_k = seq_len_v = n_nodes for decoder\n",
        "        \"\"\"\n",
        "\n",
        "        compatibility = tf.matmul(Q, K, transpose_b=True)/tf.math.sqrt(self.dk_mha_decoder)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "\n",
        "        if mask is not None:\n",
        "\n",
        "            # we need to reshape mask:\n",
        "            # (batch_size, seq_len_q, seq_len_k) --> (batch_size, 1, seq_len_q, seq_len_k)\n",
        "            # so that we will be able to do a broadcast:\n",
        "            # (batch_size, num_heads, seq_len_q, seq_len_k) + (batch_size, 1, seq_len_q, seq_len_k)\n",
        "            mask = mask[:, tf.newaxis, :, :]\n",
        "\n",
        "            # we use tf.where since 0*-np.inf returns nan, but not -np.inf\n",
        "            # compatibility = tf.where(\n",
        "            #                     tf.broadcast_to(mask, compatibility.shape), tf.ones_like(compatibility) * (-np.inf),\n",
        "            #                     compatibility\n",
        "            #                      )\n",
        "\n",
        "            compatibility = tf.where(mask,\n",
        "                                     tf.ones_like(compatibility) * (-np.inf),\n",
        "                                     compatibility\n",
        "                                     )\n",
        "\n",
        "\n",
        "        compatibility = tf.nn.softmax(compatibility, axis=-1)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        attention = tf.matmul(compatibility, V)  # (batch_size, num_heads, seq_len_q, head_depth)\n",
        "\n",
        "        # transpose back to (batch_size, seq_len_q, num_heads, depth)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # concatenate heads (last 2 dimensions)\n",
        "        attention = tf.reshape(attention, (self.batch_size, -1, self.output_dim))  # (batch_size, seq_len_q, output_dim)\n",
        "\n",
        "        output = self.w_out(attention)  # (batch_size, seq_len_q, output_dim), seq_len_q = 1 for context att in decoder\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_log_p(self, Q, K, mask=None):\n",
        "        \"\"\"Single-Head attention sublayer in decoder,\n",
        "        computes log-probabilities for node selection.\n",
        "\n",
        "        Args:\n",
        "            mask: mask for nodes\n",
        "            Q: query (output of mha layer)\n",
        "                    has shape (batch_size, seq_len_q, output_dim), seq_len_q = 1 for context attention in decoder\n",
        "            K: key (projection of node embeddings)\n",
        "                    has shape  (batch_size, seq_len_k, output_dim), seq_len_k = n_nodes for decoder\n",
        "        \"\"\"\n",
        "\n",
        "        compatibility = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(self.dk_get_loc_p)\n",
        "        compatibility = tf.math.tanh(compatibility) * self.tanh_clipping\n",
        "\n",
        "        if mask is not None:\n",
        "\n",
        "            # we dont need to reshape mask like we did in multi-head version:\n",
        "            # (batch_size, seq_len_q, seq_len_k) --> (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "            # since we dont have multiple heads\n",
        "\n",
        "            # compatibility = tf.where(\n",
        "            #                     tf.broadcast_to(mask, compatibility.shape), tf.ones_like(compatibility) * (-np.inf),\n",
        "            #                     compatibility\n",
        "            #                      )\n",
        "\n",
        "            compatibility = tf.where(mask,\n",
        "                                     tf.ones_like(compatibility) * (-np.inf),\n",
        "                                     compatibility\n",
        "                                     )\n",
        "\n",
        "        log_p = tf.nn.log_softmax(compatibility, axis=-1)  # (batch_size, seq_len_q, seq_len_k)\n",
        "\n",
        "        return log_p\n",
        "\n",
        "    def get_log_likelihood(self, _log_p, a):\n",
        "\n",
        "        # Get log_p corresponding to selected actions\n",
        "        log_p = tf.gather_nd(_log_p, tf.cast(tf.expand_dims(a, axis=-1), tf.int32), batch_dims=2)\n",
        "\n",
        "        # Calculate log_likelihood\n",
        "        return tf.reduce_sum(log_p,1)\n",
        "\n",
        "    def get_projections(self, embeddings, context_vectors):\n",
        "\n",
        "        # we compute some projections (common for each policy step) before decoding loop for efficiency\n",
        "        K = self.wk(embeddings)  # (batch_size, n_nodes, output_dim)\n",
        "        K_tanh = self.wk_tanh(embeddings)  # (batch_size, n_nodes, output_dim)\n",
        "        V = self.wv(embeddings)  # (batch_size, n_nodes, output_dim)\n",
        "        Q_context = self.wq_context(context_vectors[:, tf.newaxis, :])  # (batch_size, 1, output_dim)\n",
        "\n",
        "        # we dont need to split K_tanh since there is only 1 head; Q will be split in decoding loop\n",
        "        K = self.split_heads(K, self.batch_size)  # (batch_size, num_heads, n_nodes, head_depth)\n",
        "        V = self.split_heads(V, self.batch_size)  # (batch_size, num_heads, n_nodes, head_depth)\n",
        "\n",
        "        return K_tanh, Q_context, K, V\n",
        "\n",
        "    def call(self, inputs, return_pi=False):\n",
        "\n",
        "        embeddings, mean_graph_emb = self.embedder(inputs)\n",
        "\n",
        "        self.batch_size = tf.shape(embeddings)[0]\n",
        "\n",
        "        outputs = []\n",
        "        sequences = []\n",
        "\n",
        "        state = self.problem(inputs)\n",
        "\n",
        "        K_tanh, Q_context, K, V = self.get_projections(embeddings, mean_graph_emb)\n",
        "\n",
        "        # Perform decoding steps\n",
        "        i = 0\n",
        "        inner_i = 0\n",
        "\n",
        "        while not state.all_finished():\n",
        "\n",
        "            if i > 0:\n",
        "                state.i = tf.zeros(1, dtype=tf.int64)\n",
        "                att_mask, cur_num_nodes = state.get_att_mask()\n",
        "                embeddings, context_vectors = self.embedder(inputs, att_mask, cur_num_nodes)\n",
        "                K_tanh, Q_context, K, V = self.get_projections(embeddings, context_vectors)\n",
        "\n",
        "            inner_i = 0\n",
        "            while not state.partial_finished():\n",
        "\n",
        "                step_context = self.get_step_context(state, embeddings)  # (batch_size, 1), (batch_size, 1, input_dim + 1)\n",
        "                Q_step_context = self.wq_step_context(step_context)  # (batch_size, 1, output_dim)\n",
        "                Q = Q_context + Q_step_context\n",
        "\n",
        "                # split heads for Q\n",
        "                Q = self.split_heads(Q, self.batch_size)  # (batch_size, num_heads, 1, head_depth)\n",
        "\n",
        "                # get current mask\n",
        "                mask = state.get_mask()  # (batch_size, 1, n_nodes) with True/False indicating where agent can go\n",
        "\n",
        "                # compute MHA decoder vectors for current mask\n",
        "                mha = self.decoder_mha(Q, K, V, mask)  # (batch_size, 1, output_dim)\n",
        "\n",
        "                # compute probabilities\n",
        "                log_p = self.get_log_p(mha, K_tanh, mask)  # (batch_size, 1, n_nodes)\n",
        "\n",
        "                # next step is to select node\n",
        "                selected = self._select_node(log_p)\n",
        "\n",
        "                state.step(selected)\n",
        "\n",
        "                outputs.append(log_p[:, 0, :])\n",
        "                sequences.append(selected)\n",
        "\n",
        "                inner_i += 1\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        _log_p, pi = tf.stack(outputs, 1), tf.cast(tf.stack(sequences, 1), tf.float32)\n",
        "\n",
        "        cost = self.problem.get_costs(inputs, pi)\n",
        "\n",
        "        ll = self.get_log_likelihood(_log_p, pi)\n",
        "\n",
        "        if return_pi:\n",
        "            return cost, ll, pi\n",
        "\n",
        "        return cost, ll\n"
      ],
      "metadata": {
        "id": "sFOChbjzn8zJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "def create_data_on_disk(graph_size, num_samples, is_save=True, filename=None, is_return=False, seed=1234):\n",
        "    \"\"\"Generate validation dataset (with SEED) and save\n",
        "    \"\"\"\n",
        "\n",
        "    CAPACITIES = {\n",
        "        10: 20.,\n",
        "        20: 30.,\n",
        "        50: 40.,\n",
        "        100: 50.\n",
        "    }\n",
        "    depo, graphs, demand = (tf.random.uniform(minval=0, maxval=1, shape=(num_samples, 2), seed=seed),\n",
        "                            tf.random.uniform(minval=0, maxval=1, shape=(num_samples, graph_size, 2), seed=seed),\n",
        "                            tf.cast(tf.random.uniform(minval=1, maxval=10, shape=(num_samples, graph_size),\n",
        "                                                      dtype=tf.int32, seed=seed), tf.float32) / tf.cast(CAPACITIES[graph_size], tf.float32)\n",
        "                            )\n",
        "    if is_save:\n",
        "        save_to_pickle('Validation_dataset_{}.pkl'.format(filename), (depo, graphs, demand))\n",
        "\n",
        "    if is_return:\n",
        "        return tf.data.Dataset.from_tensor_slices((list(depo), list(graphs), list(demand)))\n",
        "\n",
        "\n",
        "def save_to_pickle(filename, item):\n",
        "    \"\"\"Save to pickle\n",
        "    \"\"\"\n",
        "    with open(filename, 'wb') as handle:\n",
        "        pickle.dump(item, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def read_from_pickle(path, return_tf_data_set=True, num_samples=None):\n",
        "    \"\"\"Read dataset from file (pickle)\n",
        "    \"\"\"\n",
        "\n",
        "    objects = []\n",
        "    with (open(path, \"rb\")) as openfile:\n",
        "        while True:\n",
        "            try:\n",
        "                objects.append(pickle.load(openfile))\n",
        "            except EOFError:\n",
        "                break\n",
        "    objects = objects[0]\n",
        "    if return_tf_data_set:\n",
        "        depo, graphs, demand = objects\n",
        "        if num_samples is not None:\n",
        "            return tf.data.Dataset.from_tensor_slices((list(depo), list(graphs), list(demand))).take(num_samples)\n",
        "        else:\n",
        "            return tf.data.Dataset.from_tensor_slices((list(depo), list(graphs), list(demand)))\n",
        "    else:\n",
        "        return objects\n",
        "\n",
        "\n",
        "def generate_data_onfly(num_samples=10000, graph_size=20):\n",
        "    \"\"\"Generate temp dataset in memory\n",
        "    \"\"\"\n",
        "\n",
        "    CAPACITIES = {\n",
        "        10: 20.,\n",
        "        20: 30.,\n",
        "        50: 40.,\n",
        "        100: 50.\n",
        "    }\n",
        "    depo, graphs, demand = (tf.random.uniform(minval=0, maxval=1, shape=(num_samples, 2)),\n",
        "                            tf.random.uniform(minval=0, maxval=1, shape=(num_samples, graph_size, 2)),\n",
        "                            tf.cast(tf.random.uniform(minval=1, maxval=10, shape=(num_samples, graph_size),\n",
        "                                                      dtype=tf.int32), tf.float32)/tf.cast(CAPACITIES[graph_size], tf.float32)\n",
        "                            )\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices((list(depo), list(graphs), list(demand)))\n",
        "\n",
        "\n",
        "def get_results(train_loss_results, train_cost_results, val_cost, save_results=True, filename=None, plots=True):\n",
        "\n",
        "    epochs_num = len(train_loss_results)\n",
        "\n",
        "    df_train = pd.DataFrame(data={'epochs': list(range(epochs_num)),\n",
        "                                  'loss': train_loss_results,\n",
        "                                  'cost': train_cost_results,\n",
        "                                  })\n",
        "    df_test = pd.DataFrame(data={'epochs': list(range(epochs_num)),\n",
        "                                 'val_сost': val_cost})\n",
        "    if save_results:\n",
        "        df_train.to_excel('train_results_{}.xlsx'.format(filename), index=False)\n",
        "        df_test.to_excel('test_results_{}.xlsx'.format(filename), index=False)\n",
        "\n",
        "    if plots:\n",
        "        plt.figure(figsize=(15, 9))\n",
        "        ax = sns.lineplot(x='epochs', y='loss', data=df_train, color='salmon', label='train loss')\n",
        "        ax2 = ax.twinx()\n",
        "        sns.lineplot(x='epochs', y='cost', data=df_train, color='cornflowerblue', label='train cost', ax=ax2)\n",
        "        sns.lineplot(x='epochs', y='val_сost', data=df_test, palette='darkblue', label='val cost').set(ylabel='cost')\n",
        "        ax.legend(loc=(0.75, 0.90), ncol=1)\n",
        "        ax2.legend(loc=(0.75, 0.95), ncol=2)\n",
        "        ax.grid(axis='x')\n",
        "        ax2.grid(True)\n",
        "        plt.savefig('learning_curve_plot_{}.jpg'.format(filename))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def get_journey(batch, pi, title, ind_in_batch=0):\n",
        "    \"\"\"Plots journey of agent\n",
        "\n",
        "    Args:\n",
        "        batch: dataset of graphs\n",
        "        pi: paths of agent obtained from model\n",
        "        ind_in_batch: index of graph in batch to be plotted\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove extra zeros\n",
        "    pi_ = get_clean_path(pi[ind_in_batch].numpy())\n",
        "\n",
        "    # Unpack variables\n",
        "    depo_coord = batch[0][ind_in_batch].numpy()\n",
        "    points_coords = batch[1][ind_in_batch].numpy()\n",
        "    demands = batch[2][ind_in_batch].numpy()\n",
        "    node_labels = ['(' + str(x[0]) + ', ' + x[1] + ')' for x in enumerate(demands.round(2).astype(str))]\n",
        "\n",
        "    # Concatenate depot and points\n",
        "    full_coords = np.concatenate((depo_coord.reshape(1, 2), points_coords))\n",
        "\n",
        "    # Get list with agent loops in path\n",
        "    list_of_paths = []\n",
        "    cur_path = []\n",
        "    for idx, node in enumerate(pi_):\n",
        "\n",
        "        cur_path.append(node)\n",
        "\n",
        "        if idx != 0 and node == 0:\n",
        "            if cur_path[0] != 0:\n",
        "                cur_path.insert(0, 0)\n",
        "            list_of_paths.append(cur_path)\n",
        "            cur_path = []\n",
        "\n",
        "    list_of_path_traces = []\n",
        "    for path_counter, path in enumerate(list_of_paths):\n",
        "        coords = full_coords[[int(x) for x in path]]\n",
        "\n",
        "        # Calculate length of each agent loop\n",
        "        lengths = np.sqrt(np.sum(np.diff(coords, axis=0) ** 2, axis=1))\n",
        "        total_length = np.sum(lengths)\n",
        "\n",
        "        list_of_path_traces.append(go.Scatter(x=coords[:, 0],\n",
        "                                              y=coords[:, 1],\n",
        "                                              mode=\"markers+lines\",\n",
        "                                              name=f\"path_{path_counter}, length={total_length:.2f}\",\n",
        "                                              opacity=1.0))\n",
        "\n",
        "    trace_points = go.Scatter(x=points_coords[:, 0],\n",
        "                              y=points_coords[:, 1],\n",
        "                              mode='markers+text',\n",
        "                              name='destinations',\n",
        "                              text=node_labels,\n",
        "                              textposition='top center',\n",
        "                              marker=dict(size=7),\n",
        "                              opacity=1.0\n",
        "                              )\n",
        "\n",
        "    trace_depo = go.Scatter(x=[depo_coord[0]],\n",
        "                            y=[depo_coord[1]],\n",
        "                            text=['1.0'], textposition='bottom center',\n",
        "                            mode='markers+text',\n",
        "                            marker=dict(size=15),\n",
        "                            name='depot'\n",
        "                            )\n",
        "\n",
        "    layout = go.Layout(title='<b>Example: {}</b>'.format(title),\n",
        "                       xaxis=dict(title='X coordinate'),\n",
        "                       yaxis=dict(title='Y coordinate'),\n",
        "                       showlegend=True,\n",
        "                       width=1000,\n",
        "                       height=1000,\n",
        "                       template=\"plotly_white\"\n",
        "                       )\n",
        "\n",
        "    data = [trace_points, trace_depo] + list_of_path_traces\n",
        "    print('Current path: ', pi_)\n",
        "    fig = go.Figure(data=data, layout=layout)\n",
        "    fig.show()\n",
        "\n",
        "def get_cur_time():\n",
        "    \"\"\"Returns local time as string\n",
        "    \"\"\"\n",
        "    ts = time.time()\n",
        "    return datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "\n",
        "def get_clean_path(arr):\n",
        "    \"\"\"Returns extra zeros from path.\n",
        "       Dynamical model generates duplicated zeros for several graphs when obtaining partial solutions.\n",
        "    \"\"\"\n",
        "\n",
        "    p1, p2 = 0, 1\n",
        "    output = []\n",
        "\n",
        "    while p2 < len(arr):\n",
        "\n",
        "        if arr[p1] != arr[p2]:\n",
        "            output.append(arr[p1])\n",
        "            if p2 == len(arr) - 1:\n",
        "                output.append(arr[p2])\n",
        "\n",
        "        p1 += 1\n",
        "        p2 += 1\n",
        "\n",
        "    if output[0] != 0:\n",
        "        output.insert(0, 0.0)\n",
        "    if output[-1] != 0:\n",
        "        output.append(0.0)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "E1mXgPAeodJX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from scipy.stats import ttest_rel\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# from attention_dynamic_model import AttentionDynamicModel\n",
        "# from attention_dynamic_model import set_decode_type\n",
        "# from utils import generate_data_onfly\n",
        "\n",
        "\n",
        "def copy_of_tf_model(model, embedding_dim=128, graph_size=20):\n",
        "    \"\"\"Copy model weights to new model\n",
        "    \"\"\"\n",
        "    # https://stackoverflow.com/questions/56841736/how-to-copy-a-network-in-tensorflow-2-0\n",
        "    CAPACITIES = {10: 20.,\n",
        "                  20: 30.,\n",
        "                  50: 40.,\n",
        "                  100: 50.\n",
        "                  }\n",
        "\n",
        "    data_random = [tf.random.uniform((2, 2,), minval=0, maxval=1, dtype=tf.dtypes.float32),\n",
        "                   tf.random.uniform((2, graph_size, 2), minval=0, maxval=1, dtype=tf.dtypes.float32),\n",
        "                   tf.cast(tf.random.uniform(minval=1, maxval=10, shape=(2, graph_size),\n",
        "                                             dtype=tf.int32), tf.float32) / tf.cast(CAPACITIES[graph_size], tf.float32)]\n",
        "\n",
        "    new_model = AttentionDynamicModel(embedding_dim)\n",
        "    set_decode_type(new_model, \"sampling\")\n",
        "    _, _ = new_model(data_random)\n",
        "\n",
        "    for a, b in zip(new_model.variables, model.variables):\n",
        "        a.assign(b)\n",
        "\n",
        "    return new_model\n",
        "\n",
        "def rollout(model, dataset, batch_size = 1000, disable_tqdm = False):\n",
        "    # Evaluate model in greedy mode\n",
        "    set_decode_type(model, \"greedy\")\n",
        "    costs_list = []\n",
        "\n",
        "    for batch in tqdm(dataset.batch(batch_size), disable=disable_tqdm, desc=\"Rollout greedy execution\"):\n",
        "        cost, _ = model(batch)\n",
        "        costs_list.append(cost)\n",
        "\n",
        "    return tf.concat(costs_list, axis=0)\n",
        "\n",
        "\n",
        "def validate(dataset, model, batch_size=1000):\n",
        "    \"\"\"Validates model on given dataset in greedy mode\n",
        "    \"\"\"\n",
        "    val_costs = rollout(model, dataset, batch_size=batch_size)\n",
        "    set_decode_type(model, \"sampling\")\n",
        "    mean_cost = tf.reduce_mean(val_costs)\n",
        "    print(f\"Validation score: {np.round(mean_cost, 4)}\")\n",
        "    return mean_cost\n",
        "\n",
        "\n",
        "class RolloutBaseline:\n",
        "\n",
        "    def __init__(self, model, filename,\n",
        "                 from_checkpoint=False,\n",
        "                 path_to_checkpoint=None,\n",
        "                 wp_n_epochs=1,\n",
        "                 epoch=0,\n",
        "                 num_samples=10000,\n",
        "                 warmup_exp_beta=0.8,\n",
        "                 embedding_dim=128,\n",
        "                 graph_size=20\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: current model\n",
        "            filename: suffix for baseline checkpoint filename\n",
        "            from_checkpoint: start from checkpoint flag\n",
        "            path_to_checkpoint: path to baseline model weights\n",
        "            wp_n_epochs: number of warm-up epochs\n",
        "            epoch: current epoch number\n",
        "            num_samples: number of samples to be generated for baseline dataset\n",
        "            warmup_exp_beta: warmup mixing parameter (exp. moving average parameter)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "        self.cur_epoch = epoch\n",
        "        self.wp_n_epochs = wp_n_epochs\n",
        "        self.beta = warmup_exp_beta\n",
        "\n",
        "        # controls the amount of warmup\n",
        "        self.alpha = 0.0\n",
        "\n",
        "        self.running_average_cost = None\n",
        "\n",
        "        # Checkpoint params\n",
        "        self.filename = filename\n",
        "        self.from_checkpoint = from_checkpoint\n",
        "        self.path_to_checkpoint = path_to_checkpoint\n",
        "\n",
        "        # Problem params\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.graph_size = graph_size\n",
        "\n",
        "        # create and evaluate initial baseline\n",
        "        self._update_baseline(model, epoch)\n",
        "\n",
        "\n",
        "    def _update_baseline(self, model, epoch):\n",
        "\n",
        "        # Load or copy baseline model based on self.from_checkpoint condition\n",
        "        if self.from_checkpoint and self.alpha == 0:\n",
        "            print('Baseline model loaded')\n",
        "            self.model = load_tf_model(self.path_to_checkpoint,\n",
        "                                       embedding_dim=self.embedding_dim,\n",
        "                                       graph_size=self.graph_size)\n",
        "        else:\n",
        "            self.model = copy_of_tf_model(model,\n",
        "                                          embedding_dim=self.embedding_dim,\n",
        "                                          graph_size=self.graph_size)\n",
        "\n",
        "            # For checkpoint\n",
        "            self.model.save_weights('baseline_checkpoint_epoch_{}_{}.h5'.format(epoch, self.filename), save_format='h5')\n",
        "\n",
        "        # We generate a new dataset for baseline model on each baseline update to prevent possible overfitting\n",
        "        self.dataset = generate_data_onfly(num_samples=self.num_samples, graph_size=self.graph_size)\n",
        "\n",
        "        print(f\"Evaluating baseline model on baseline dataset (epoch = {epoch})\")\n",
        "        self.bl_vals = rollout(self.model, self.dataset)\n",
        "        self.mean = tf.reduce_mean(self.bl_vals)\n",
        "        self.cur_epoch = epoch\n",
        "\n",
        "    def ema_eval(self, cost):\n",
        "        \"\"\"This is running average of cost through previous batches (only for warm-up epochs)\n",
        "        \"\"\"\n",
        "\n",
        "        if self.running_average_cost is None:\n",
        "            self.running_average_cost = tf.reduce_mean(cost)\n",
        "        else:\n",
        "            self.running_average_cost = self.beta * self.running_average_cost + (1. - self.beta) * tf.reduce_mean(cost)\n",
        "\n",
        "        return self.running_average_cost\n",
        "\n",
        "    def eval(self, batch, cost):\n",
        "        \"\"\"Evaluates current baseline model on single training batch\n",
        "        \"\"\"\n",
        "\n",
        "        if self.alpha == 0:\n",
        "            return self.ema_eval(cost)\n",
        "\n",
        "        if self.alpha < 1:\n",
        "            v_ema = self.ema_eval(cost)\n",
        "        else:\n",
        "            v_ema = 0.0\n",
        "\n",
        "        v_b, _ = self.model(batch)\n",
        "\n",
        "        v_b = tf.stop_gradient(v_b)\n",
        "        v_ema = tf.stop_gradient(v_ema)\n",
        "\n",
        "        # Combination of baseline cost and exp. moving average cost\n",
        "        return self.alpha * v_b + (1 - self.alpha) * v_ema\n",
        "\n",
        "    def eval_all(self, dataset):\n",
        "        \"\"\"Evaluates current baseline model on the whole dataset only for non warm-up epochs\n",
        "        \"\"\"\n",
        "\n",
        "        if self.alpha < 1:\n",
        "            return None\n",
        "\n",
        "        val_costs = rollout(self.model, dataset, batch_size=2048)\n",
        "\n",
        "        return val_costs\n",
        "\n",
        "    def epoch_callback(self, model, epoch):\n",
        "        \"\"\"Compares current baseline model with the training model and updates baseline if it is improved\n",
        "        \"\"\"\n",
        "\n",
        "        self.cur_epoch = epoch\n",
        "\n",
        "        print(f\"Evaluating candidate model on baseline dataset (callback epoch = {self.cur_epoch})\")\n",
        "        candidate_vals = rollout(model, self.dataset)  # costs for training model on baseline dataset\n",
        "        candidate_mean = tf.reduce_mean(candidate_vals)\n",
        "\n",
        "        diff = candidate_mean - self.mean\n",
        "\n",
        "        print(f\"Epoch {self.cur_epoch} candidate mean {candidate_mean}, baseline epoch {self.cur_epoch} mean {self.mean}, difference {diff}\")\n",
        "\n",
        "        if diff < 0:\n",
        "            # statistic + p-value\n",
        "            t, p = ttest_rel(candidate_vals, self.bl_vals)\n",
        "\n",
        "            p_val = p / 2\n",
        "            print(f\"p-value: {p_val}\")\n",
        "\n",
        "            if p_val < 0.05:\n",
        "                print('Update baseline')\n",
        "                self._update_baseline(model, self.cur_epoch)\n",
        "\n",
        "        # alpha controls the amount of warmup\n",
        "        if self.alpha < 1.0:\n",
        "            self.alpha = (self.cur_epoch + 1) / float(self.wp_n_epochs)\n",
        "            print(f\"alpha was updated to {self.alpha}\")\n",
        "\n",
        "\n",
        "def load_tf_model(path, embedding_dim=128, graph_size=20, n_encode_layers=2):\n",
        "    \"\"\"Load model weights from hd5 file\n",
        "    \"\"\"\n",
        "    # https://stackoverflow.com/questions/51806852/cant-save-custom-subclassed-model\n",
        "    CAPACITIES = {10: 20.,\n",
        "                  20: 30.,\n",
        "                  50: 40.,\n",
        "                  100: 50.\n",
        "                  }\n",
        "\n",
        "    data_random = [tf.random.uniform((2, 2,), minval=0, maxval=1, dtype=tf.dtypes.float32),\n",
        "                   tf.random.uniform((2, graph_size, 2), minval=0, maxval=1, dtype=tf.dtypes.float32),\n",
        "                   tf.cast(tf.random.uniform(minval=1, maxval=10, shape=(2, graph_size),\n",
        "                                             dtype=tf.int32), tf.float32) / tf.cast(CAPACITIES[graph_size], tf.float32)]\n",
        "\n",
        "    model_loaded = AttentionDynamicModel(embedding_dim,n_encode_layers=n_encode_layers)\n",
        "    set_decode_type(model_loaded, \"greedy\")\n",
        "    _, _ = model_loaded(data_random)\n",
        "\n",
        "    model_loaded.load_weights(path)\n",
        "\n",
        "    return model_loaded"
      ],
      "metadata": {
        "id": "BrA_8z0zoZCT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# from attention_dynamic_model import set_decode_type\n",
        "# from reinforce_baseline import validate\n",
        "\n",
        "# from utils import generate_data_onfly, get_results, get_cur_time\n",
        "from time import gmtime, strftime\n",
        "\n",
        "\n",
        "def train_model(optimizer,\n",
        "                model_tf,\n",
        "                baseline,\n",
        "                validation_dataset,\n",
        "                samples = 1280000,\n",
        "                batch = 128,\n",
        "                val_batch_size = 1000,\n",
        "                start_epoch = 0,\n",
        "                end_epoch = 5,\n",
        "                from_checkpoint = False,\n",
        "                grad_norm_clipping = 1.0,\n",
        "                batch_verbose = 1000,\n",
        "                graph_size = 20,\n",
        "                filename = None\n",
        "                ):\n",
        "\n",
        "    if filename is None:\n",
        "        filename = 'VRP_{}_{}'.format(graph_size, strftime(\"%Y-%m-%d\", gmtime()))\n",
        "\n",
        "    def rein_loss(model, inputs, baseline, num_batch):\n",
        "        \"\"\"Calculate loss for REINFORCE algorithm\n",
        "        \"\"\"\n",
        "\n",
        "        # Evaluate model, get costs and log probabilities\n",
        "        cost, log_likelihood = model(inputs)\n",
        "\n",
        "        # Evaluate baseline\n",
        "        # For first wp_n_epochs we take the combination of baseline and ema for previous batches\n",
        "        # after that we take a slice of precomputed baseline values\n",
        "        bl_val = bl_vals[num_batch] if bl_vals is not None else baseline.eval(inputs, cost)\n",
        "        bl_val = tf.stop_gradient(bl_val)\n",
        "\n",
        "        # Calculate loss\n",
        "        reinforce_loss = tf.reduce_mean((cost - bl_val) * log_likelihood)\n",
        "\n",
        "        return reinforce_loss, tf.reduce_mean(cost)\n",
        "\n",
        "    def grad(model, inputs, baseline, num_batch):\n",
        "        \"\"\"Calculate gradients\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss, cost = rein_loss(model, inputs, baseline, num_batch)\n",
        "        return loss, cost, tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # For plotting\n",
        "    train_loss_results = []\n",
        "    train_cost_results = []\n",
        "    val_cost_avg = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, end_epoch):\n",
        "\n",
        "        # Create dataset on current epoch\n",
        "        data = generate_data_onfly(num_samples=samples, graph_size=graph_size)\n",
        "\n",
        "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "        epoch_cost_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "        # Skip warm-up stage when we continue training from checkpoint\n",
        "        if from_checkpoint and baseline.alpha != 1.0:\n",
        "            print('Skipping warm-up mode')\n",
        "            baseline.alpha = 1.0\n",
        "\n",
        "        # If epoch > wp_n_epochs then precompute baseline values for the whole dataset else None\n",
        "        bl_vals = baseline.eval_all(data)  # (samples, ) or None\n",
        "        bl_vals = tf.reshape(bl_vals, (-1, batch)) if bl_vals is not None else None # (n_batches, batch) or None\n",
        "\n",
        "        print(\"Current decode type: {}\".format(model_tf.decode_type))\n",
        "\n",
        "        for num_batch, x_batch in tqdm(enumerate(data.batch(batch)), desc=\"batch calculation at epoch {}\".format(epoch)):\n",
        "\n",
        "            # Optimize the model\n",
        "            loss_value, cost_val, grads = grad(model_tf, x_batch, baseline, num_batch)\n",
        "\n",
        "            # Clip gradients by grad_norm_clipping\n",
        "            init_global_norm = tf.linalg.global_norm(grads)\n",
        "            grads, _ = tf.clip_by_global_norm(grads, grad_norm_clipping)\n",
        "            global_norm = tf.linalg.global_norm(grads)\n",
        "\n",
        "            if num_batch%batch_verbose == 0:\n",
        "                print(\"grad_global_norm = {}, clipped_norm = {}\".format(init_global_norm.numpy(), global_norm.numpy()))\n",
        "\n",
        "            optimizer.apply_gradients(zip(grads, model_tf.trainable_variables))\n",
        "\n",
        "            # Track progress\n",
        "            epoch_loss_avg.update_state(loss_value)\n",
        "            epoch_cost_avg.update_state(cost_val)\n",
        "\n",
        "            if num_batch%batch_verbose == 0:\n",
        "                print(\"Epoch {} (batch = {}): Loss: {}: Cost: {}\".format(epoch, num_batch, epoch_loss_avg.result(), epoch_cost_avg.result()))\n",
        "\n",
        "        # Update baseline if the candidate model is good enough. In this case also create new baseline dataset\n",
        "        baseline.epoch_callback(model_tf, epoch)\n",
        "        set_decode_type(model_tf, \"sampling\")\n",
        "\n",
        "        # Save model weights\n",
        "        model_tf.save_weights('model_checkpoint_epoch_{}_{}.h5'.format(epoch, filename), save_format='h5')\n",
        "\n",
        "        # Validate current model\n",
        "        val_cost = validate(validation_dataset, model_tf, val_batch_size)\n",
        "        val_cost_avg.append(val_cost)\n",
        "\n",
        "        train_loss_results.append(epoch_loss_avg.result())\n",
        "        train_cost_results.append(epoch_cost_avg.result())\n",
        "\n",
        "        pd.DataFrame(data={'epochs': list(range(start_epoch, epoch+1)),\n",
        "                           'train_loss': [x.numpy() for x in train_loss_results],\n",
        "                           'train_cost': [x.numpy() for x in train_cost_results],\n",
        "                           'val_cost': [x.numpy() for x in val_cost_avg]\n",
        "                           }).to_csv('backup_results_' + filename + '.csv', index=False)\n",
        "\n",
        "        print(get_cur_time(), \"Epoch {}: Loss: {}: Cost: {}\".format(epoch, epoch_loss_avg.result(), epoch_cost_avg.result()))\n",
        "\n",
        "    # Make plots and save results\n",
        "    filename_for_results = filename + '_start={}, end={}'.format(start_epoch, end_epoch)\n",
        "    get_results([x.numpy() for x in train_loss_results],\n",
        "                [x.numpy() for x in train_cost_results],\n",
        "                [x.numpy() for x in val_cost_avg],\n",
        "                save_results=True,\n",
        "                filename=filename_for_results,\n",
        "                plots=True)"
      ],
      "metadata": {
        "id": "SpgNdA9Solw3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aRJY-NgSn0oq",
        "outputId": "bf4fbb40-fdbe-491c-bacb-7f61976e4374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-03 03:36:37 model initialized\n",
            "2022-06-03 03:36:50 validation dataset created and saved on the disk\n",
            "Evaluating baseline model on baseline dataset (epoch = 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [14:21<00:00, 86.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-03 03:51:29 baseline initialized\n",
            "Current decode type: sampling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 0: 1it [00:20, 20.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_global_norm = 9.431756973266602, clipped_norm = 1.0\n",
            "Epoch 0 (batch = 0): Loss: -1.6993656158447266: Cost: 30.612022399902344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 0: 4it [00:52, 13.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [02:38<00:00, 15.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 candidate mean 21.130067825317383, baseline epoch 0 mean 52.03078079223633, difference -30.900712966918945\n",
            "p-value: 0.0\n",
            "Update baseline\n",
            "Evaluating baseline model on baseline dataset (epoch = 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [03:21<00:00, 20.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpha was updated to 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [03:21<00:00, 20.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 21.138599395751953\n",
            "2022-06-03 04:01:51 Epoch 0: Loss: 15.752889633178711: Cost: 30.426376342773438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:08<00:00,  8.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 1: 1it [00:12, 12.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_global_norm = 25.53541374206543, clipped_norm = 0.9999999403953552\n",
            "Epoch 1 (batch = 0): Loss: -1178.719970703125: Cost: 29.57387351989746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 1: 4it [00:42, 10.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [03:21<00:00, 20.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 candidate mean 21.164522171020508, baseline epoch 1 mean 21.10788917541504, difference 0.05663299560546875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [02:41<00:00, 16.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 21.193899154663086\n",
            "2022-06-03 04:09:26 Epoch 1: Loss: -1191.518798828125: Cost: 29.597671508789062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:10<00:00, 10.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 2: 1it [00:09,  9.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_global_norm = 46.13264465332031, clipped_norm = 1.0\n",
            "Epoch 2 (batch = 0): Loss: -1080.165771484375: Cost: 28.921009063720703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 2: 4it [00:41, 10.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [02:45<00:00, 16.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 candidate mean 21.273801803588867, baseline epoch 2 mean 21.10788917541504, difference 0.16591262817382812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [03:21<00:00, 20.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 21.30470085144043\n",
            "2022-06-03 04:16:25 Epoch 2: Loss: -1018.86083984375: Cost: 28.591703414916992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:08<00:00,  8.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 3: 1it [00:09,  9.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_global_norm = 15.258009910583496, clipped_norm = 1.0\n",
            "Epoch 3 (batch = 0): Loss: -1026.7474365234375: Cost: 28.44186019897461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 3: 4it [00:37,  9.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [03:21<00:00, 20.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 candidate mean 21.26430320739746, baseline epoch 3 mean 21.10788917541504, difference 0.15641403198242188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [03:21<00:00, 20.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 21.311599731445312\n",
            "2022-06-03 04:23:59 Epoch 3: Loss: -959.6767578125: Cost: 28.10523223876953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 1/1 [00:10<00:00, 10.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current decode type: sampling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 4: 1it [00:13, 13.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad_global_norm = 14.38542366027832, clipped_norm = 0.9999999403953552\n",
            "Epoch 4 (batch = 0): Loss: -897.591796875: Cost: 27.57366180419922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "batch calculation at epoch 4: 4it [00:42, 10.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating candidate model on baseline dataset (callback epoch = 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [02:43<00:00, 16.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 candidate mean 21.284080505371094, baseline epoch 4 mean 21.10788917541504, difference 0.1761913299560547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Rollout greedy execution: 100%|██████████| 10/10 [03:21<00:00, 20.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score: 21.31130027770996\n",
            "2022-06-03 04:31:37 Epoch 4: Loss: -886.1036987304688: Cost: 27.832061767578125\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x648 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from time import gmtime, strftime\n",
        "\n",
        "#from attention_dynamic_model import AttentionDynamicModel, set_decode_type\n",
        "# from reinforce_baseline import RolloutBaseline\n",
        "# from train import train_model\n",
        "\n",
        "# from utils import create_data_on_disk, get_cur_time\n",
        "\n",
        "# Params of model\n",
        "SAMPLES = 512 # 128*10000\n",
        "BATCH = 128\n",
        "START_EPOCH = 0\n",
        "END_EPOCH = 5\n",
        "FROM_CHECKPOINT = False\n",
        "embedding_dim = 128\n",
        "LEARNING_RATE = 0.0001\n",
        "ROLLOUT_SAMPLES = 10000\n",
        "NUMBER_OF_WP_EPOCHS = 1\n",
        "GRAD_NORM_CLIPPING = 1.0\n",
        "BATCH_VERBOSE = 1000\n",
        "VAL_BATCH_SIZE = 1000\n",
        "VALIDATE_SET_SIZE = 10000\n",
        "SEED = 1234\n",
        "GRAPH_SIZE = 50\n",
        "FILENAME = 'VRP_{}_{}'.format(GRAPH_SIZE, strftime(\"%Y-%m-%d\", gmtime()))\n",
        "\n",
        "# Initialize model\n",
        "model_tf = AttentionDynamicModel(embedding_dim)\n",
        "set_decode_type(model_tf, \"sampling\")\n",
        "print(get_cur_time(), 'model initialized')\n",
        "\n",
        "# Create and save validation dataset\n",
        "validation_dataset = create_data_on_disk(GRAPH_SIZE,\n",
        "                                         VALIDATE_SET_SIZE,\n",
        "                                         is_save=True,\n",
        "                                         filename=FILENAME,\n",
        "                                         is_return=True,\n",
        "                                         seed = SEED)\n",
        "print(get_cur_time(), 'validation dataset created and saved on the disk')\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "\n",
        "# Initialize baseline\n",
        "baseline = RolloutBaseline(model_tf,\n",
        "                           wp_n_epochs = NUMBER_OF_WP_EPOCHS,\n",
        "                           epoch = 0,\n",
        "                           num_samples=ROLLOUT_SAMPLES,\n",
        "                           filename = FILENAME,\n",
        "                           from_checkpoint = FROM_CHECKPOINT,\n",
        "                           embedding_dim=embedding_dim,\n",
        "                           graph_size=GRAPH_SIZE\n",
        "                           )\n",
        "print(get_cur_time(), 'baseline initialized')\n",
        "\n",
        "train_model(optimizer,\n",
        "            model_tf,\n",
        "            baseline,\n",
        "            validation_dataset,\n",
        "            samples = SAMPLES,\n",
        "            batch = BATCH,\n",
        "            val_batch_size = VAL_BATCH_SIZE,\n",
        "            start_epoch = START_EPOCH,\n",
        "            end_epoch = END_EPOCH,\n",
        "            from_checkpoint = FROM_CHECKPOINT,\n",
        "            grad_norm_clipping = GRAD_NORM_CLIPPING,\n",
        "            batch_verbose = BATCH_VERBOSE,\n",
        "            graph_size = GRAPH_SIZE,\n",
        "            filename = FILENAME\n",
        "            )"
      ]
    }
  ]
}